{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of timedate when it was published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Page 1 ===\n",
      "start: 0  total: 418824\n",
      "=== Page 2 ===\n",
      "start: 1000  total: 418824\n",
      "=== Page 3 ===\n",
      "start: 2000  total: 418824\n",
      "=== Page 4 ===\n",
      "start: 3000  total: 418824\n",
      "=== Page 5 ===\n",
      "start: 4000  total: 418824\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "base = 'https://dataverse.harvard.edu'\n",
    "rows = 1000\n",
    "start = 0\n",
    "page = 1\n",
    "condition = True # emulate do-while\n",
    "prueba = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=2)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "prueba.mount('http://', adapter)\n",
    "prueba.mount('https://', adapter)\n",
    "scraping_types=pd.DataFrame(columns=['Type'])\n",
    "scraping_names=pd.DataFrame(columns=['Name'])\n",
    "scraping_publication=pd.DataFrame(columns=['PublicationDate'])\n",
    "\n",
    "\n",
    "\n",
    "while (condition):\n",
    "    url = base + '/api/search?q=*&type=file&show_facets=true&per_page=1000' + \"&start=\" + str(start)\n",
    "    data = json.loads(prueba.get(url).text)\n",
    "    total = data['data']['total_count']\n",
    "    print(\"=== Page\", page, \"===\")\n",
    "    print(\"start:\", start, \" total:\", total)\n",
    "    for i in data['data']['items']:\n",
    "        scraping_types=scraping_types.append(pd.DataFrame([i['type']], columns=['Type']))\n",
    "        scraping_names=scraping_names.append(pd.DataFrame([i['name']], columns=['Name']))\n",
    "        scraping_publication=scraping_publication.append(pd.DataFrame([i['published_at']], columns=['PublicationDate']))\n",
    "\n",
    "    start = start + rows\n",
    "    page += 1\n",
    "    condition = start < total/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of file formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Page 1 ===\n",
      "start: 0  total: 418822\n",
      "    index          Name   Count\n",
      "0       0          Data  114481\n",
      "1       0       Unknown   71496\n",
      "2       0      Document   59259\n",
      "3       0  Tabular Data   47471\n",
      "4       0          Text   44846\n",
      "5       0         Image   40509\n",
      "6       0   Application   23623\n",
      "7       0           ZIP    5862\n",
      "8       0          FITS    4995\n",
      "9       0         Audio    3812\n",
      "10      0         Video    1136\n",
      "11      0         Shape     868\n",
      "12      0          text     245\n",
      "13      0          fits     144\n",
      "14      0  Network Data      55\n",
      "15      0      Chemical       9\n",
      "16      0        Binary       7\n",
      "17      0         Model       2\n",
      "18      0      document       1\n",
      "19      0       unknown       1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "base = 'https://dataverse.harvard.edu'\n",
    "rows = 10\n",
    "start = 0\n",
    "page = 1\n",
    "condition = True # emulate do-while\n",
    "prueba = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=2)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "prueba.mount('http://', adapter)\n",
    "prueba.mount('https://', adapter)\n",
    "filetypes_count=pd.DataFrame(columns=['Count'])\n",
    "filetypes_name=pd.DataFrame(columns=['Name'])\n",
    "\n",
    "url = base + '/api/search?q=*&type=file&show_facets=true&per_page=1' + \"&start=\" + str(start)\n",
    "data = json.loads(prueba.get(url).text)\n",
    "total = data['data']['total_count']\n",
    "print(\"=== Page\", page, \"===\")\n",
    "print(\"start:\", start, \" total:\", total)\n",
    "    \n",
    "\n",
    "for i in data['data']['facets']:\n",
    "    for d in i['fileTypeGroupFacet']['labels']:\n",
    "        for j in d:\n",
    "            filetypes_count=filetypes_count.append(pd.DataFrame([d[j]], columns=['Count']))\n",
    "            filetypes_name=filetypes_name.append(pd.DataFrame([j], columns=['Name']))\n",
    "\n",
    "final=pd.concat([filetypes_name,filetypes_count],axis=1).reset_index()\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.csv('fileformats.csv',sep=\";\", encoding=\"UTF-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
